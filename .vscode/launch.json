{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "env": {
                "WANDB__SERVICE_WAIT": "60",
                //"CUDA_VISIBLE_DEVICES": "4,5,6,7"  // Set this to the appropriate GPU device indices
            },
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "/home/jingran_su/miniconda3/envs/llava/bin/deepspeed", //"${file}",
            "console": "integratedTerminal",
            "args": [
                "--num_nodes=1",
                "--num_gpus=8",
                "llava_uhd/train/llava-uhd/train_mem.py",
                "--deepspeed", "./scripts/zero2.json",
                "--model_name_or_path", "lmsys/vicuna-13b-v1.5",
                "--version", "v1",
                "--data_path", "/data0/jingran/workspace/UI_training_data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json",
                "--image_folder", "/data0/jingran/workspace/UI_training_data/LLaVA-Pretrain/images",
                "--vision_tower", "openai/clip-vit-large-patch14-336",
                //"--pretrain_mm_mlp_adapter", "/home/jingran_su/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b/snapshots/12e054b30e8e061f423c7264bc97d4248232e965/mm_projector.bin",
                "--mm_projector_type", "mlp2x_gelu",
                "--tune_mm_mlp_adapter", "True",
                "--mm_vision_select_layer", "-2",
                "--mm_use_im_start_end", "False",
                "--mm_use_im_patch_token", "False",
                "--image_aspect_ratio", "pad",
                // "--group_by_modality_length", "True",
                "--bf16", "True",
                "--output_dir", "./checkpoints/llava-v1.5-13b_pretrain",
                "--num_train_epochs", "1",
                "--per_device_train_batch_size", "32",
                "--per_device_eval_batch_size", "4",
                "--gradient_accumulation_steps", "1",
                "--evaluation_strategy", "no",
                "--save_strategy", "steps",
                "--save_steps", "24",
                "--save_total_limit", "1",
                "--learning_rate", "1e-3",
                "--weight_decay", "0.",
                "--warmup_ratio", "0.03",
                "--lr_scheduler_type", "cosine",
                "--logging_steps", "1",
                "--tf32", "True",
                "--model_max_length", "2048",
                "--gradient_checkpointing", "True",
                "--dataloader_num_workers", "4",
                "--lazy_preprocess", "True",
                "--report_to", "wandb"
            ],
        }
    ]
}